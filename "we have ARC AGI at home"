#!/usr/bin/env python3
"""
ARC AGI at home:
Compatibility-fixed training script:
- MLP -> Latent Data Matrix -> MLP pipeline
- torch.amp compatibility across PyTorch versions
- CUDA + AMP if available
- precomputed one-hot dataset to minimize CPU overhead
- DataLoader pin_memory and num_workers tuning
- OOM fallback: reduce batch / use gradient accumulation
"""

import os
import time
import random
import argparse
from typing import Tuple
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch import amp
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt

# ---------------- CONFIG ----------------
CONFIG = {
    "n_examples": 120,
    "train_val_split": 0.8,
    "latent_dim": 256,
    "encoder_hidden": 1024,
    "decoder_hidden": 1024,
    "ldm_rows": 8,
    "ldm_iters": 2,
    "epochs": 8,
    "batch_size": 128,
    "lr": 1e-3,
    "weight_decay": 1e-5,
    "num_workers": 4,
    "pin_memory": True,
    "checkpoint_dir": "checkpoints",
    "checkpoint_every": 4,
    "fallback_batch_divisor": 2,
    "use_grad_accum": True,
    "grad_accum_steps": 2,
}
# -----------------------------------------

SEED = 21
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
HAS_CUDA = DEVICE.type == "cuda"
torch.backends.cudnn.benchmark = True

GRID = 6
N_COLORS = 6
TASKS = ['invert_color','mirror_h','add_border','paint_center']

def sample_task_instance(task: str, grid_size=GRID, n_colors=N_COLORS) -> Tuple[np.ndarray, np.ndarray]:
    x = np.zeros((grid_size, grid_size), dtype=np.int64)
    bg = random.randint(0, n_colors-1); x.fill(bg)
    for _ in range(random.randint(0,2)):
        color = random.randint(0, n_colors-1)
        r = random.randint(0, grid_size-1); c = random.randint(0, grid_size-1)
        h = random.randint(1, max(1, grid_size//3)); w = random.randint(1, max(1, grid_size//3))
        r2 = min(grid_size, r+h); c2 = min(grid_size, c+w)
        x[r:r2, c:c2] = color
    y = x.copy()
    if task == 'invert_color':
        a,b = random.sample(range(n_colors),2); y[x==a]=b; y[x==b]=a
    elif task == 'mirror_h':
        y = np.fliplr(x)
    elif task == 'add_border':
        bc = random.randint(0, n_colors-1); y[0,:]=bc; y[-1,:]=bc; y[:,0]=bc; y[:,-1]=bc
    elif task == 'paint_center':
        fc = random.randint(0, n_colors-1); mid = grid_size//2; y[mid-1:mid+1, mid-1:mid+1] = fc
    return x, y

def build_dataset(n_examples:int):
    X=[]; Y=[]; T=[]
    for _ in range(n_examples):
        t = random.choice(TASKS); x,y = sample_task_instance(t)
        X.append(x); Y.append(y); T.append(t)
    return np.stack(X), np.stack(Y), T

def one_hot_grid(arr: np.ndarray):
    N,H,W = arr.shape
    return np.eye(N_COLORS, dtype=np.float32)[arr.reshape(-1)].reshape(N,H,W,N_COLORS)

class PrecomputedArcDataset(Dataset):
    def __init__(self, X_onehot: np.ndarray, Y: np.ndarray):
        assert X_onehot.dtype == np.float32
        assert Y.dtype in (np.int64, np.int32)
        self.X = X_onehot
        self.Y = Y
    def __len__(self): return self.X.shape[0]
    def __getitem__(self, i):
        return torch.from_numpy(self.X[i]), torch.from_numpy(self.Y[i])

class EncoderMLP(nn.Module):
    def __init__(self, latent_dim=32, hidden=96):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(GRID*GRID*N_COLORS, hidden),
            nn.ReLU(),
            nn.Linear(hidden, latent_dim),
            nn.ReLU()
        )
    def forward(self,x): return self.net(x.view(x.size(0), -1))

class LatentDataMatrix(nn.Module):
    def __init__(self, latent_dim=32, rows=8, iters=2):
        super().__init__()
        assert latent_dim % rows == 0
        self.rows = rows; self.cols = latent_dim // rows; self.iters = iters
        self.mix = nn.Linear(self.cols, self.cols)
        self.rowmix = nn.Linear(self.rows, self.rows)
        self.cell = nn.Sequential(nn.Linear(self.cols, self.cols), nn.ReLU())
    def forward(self, latent):
        B = latent.size(0)
        mat = latent.view(B, self.rows, self.cols)
        for _ in range(self.iters):
            mat = mat + self.cell(self.mix(mat))
            mt = mat.transpose(1,2)
            mt = mt + self.rowmix(mt)
            mat = mt.transpose(1,2)
        return mat.view(B, -1)

class DecoderMLP(nn.Module):
    def __init__(self, latent_dim=32, hidden=96):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, GRID*GRID*N_COLORS)
        )
    def forward(self, z): return self.net(z).view(z.size(0), GRID, GRID, N_COLORS)

class Pipeline(nn.Module):
    def __init__(self, latent_dim=32, enc_h=96, dec_h=96, rows=8, iters=2):
        super().__init__()
        self.enc = EncoderMLP(latent_dim, enc_h)
        self.ldm = LatentDataMatrix(latent_dim, rows, iters)
        self.dec = DecoderMLP(latent_dim, dec_h)
    def forward(self,x):
        z = self.enc(x); z2 = self.ldm(z); logits = self.dec(z2); return logits

# --------- Compatibility helpers for AMP ----------
class DummyScaler:
    def scale(self, loss): return loss
    def step(self, optimizer): optimizer.step()
    def update(self): pass

def make_grad_scaler():
    # try modern API first, fallback to older or dummy if needed
    if HAS_CUDA:
        try:
            return amp.GradScaler(device_type="cuda")
        except TypeError:
            try:
                return amp.GradScaler()
            except Exception:
                return DummyScaler()
    else:
        return DummyScaler()

def autocast_cm():
    # return a context manager suitable for the runtime
    if HAS_CUDA:
        try:
            return amp.autocast(device_type="cuda")
        except TypeError:
            try:
                return amp.autocast()
            except Exception:
                # last-resort: no-op context manager
                class DummyCM:
                    def __enter__(self): return None
                    def __exit__(self, exc_type, exc, tb): return False
                return DummyCM()
    else:
        class DummyCM:
            def __enter__(self): return None
            def __exit__(self, exc_type, exc, tb): return False
        return DummyCM()

def print_gpu_mem(prefix="GPU"):
    if not HAS_CUDA:
        print(f"{prefix}: CUDA not available")
        return
    torch.cuda.synchronize()
    used = torch.cuda.memory_allocated()/(1024**3)
    reserved = torch.cuda.memory_reserved()/(1024**3)
    print(f"{prefix} memory used: {used:.3f} GB; reserved: {reserved:.3f} GB")

def prepare_dataloaders(X_all, Y_all, config):
    X_onehot = one_hot_grid(X_all)
    N = len(X_all)
    n_train = int(N*config["train_val_split"])
    train_X, val_X = X_onehot[:n_train], X_onehot[n_train:]
    train_Y, val_Y = Y_all[:n_train], Y_all[n_train:]
    train_ds = PrecomputedArcDataset(train_X, train_Y)
    val_ds = PrecomputedArcDataset(val_X, val_Y)
    dl_train = DataLoader(train_ds, batch_size=config["batch_size"], shuffle=True,
                          num_workers=config["num_workers"], pin_memory=config["pin_memory"])
    dl_val = DataLoader(val_ds, batch_size=config["batch_size"], shuffle=False,
                        num_workers=max(0, config["num_workers"]//2), pin_memory=config["pin_memory"])
    return dl_train, dl_val

def train(config):
    os.makedirs(config["checkpoint_dir"], exist_ok=True)
    print("Building dataset...")
    X_all, Y_all, T_all = build_dataset(config["n_examples"])
    dl_train, dl_val = prepare_dataloaders(X_all, Y_all, config)
    print("Train size:", len(dl_train.dataset), "Val size:", len(dl_val.dataset))
    print("Device:", DEVICE)
    # build model and move to device BEFORE optimizer
    model = Pipeline(config["latent_dim"], config["encoder_hidden"], config["decoder_hidden"],
                     config["ldm_rows"], config["ldm_iters"]).to(DEVICE)
    print_gpu_mem("After model.to()")

    optimizer = optim.AdamW(model.parameters(), lr=config["lr"], weight_decay=config["weight_decay"])
    scaler = make_grad_scaler()
    criterion = nn.CrossEntropyLoss()
    amp_ctx = autocast_cm()

    try:
        for epoch in range(1, config["epochs"]+1):
            model.train()
            running_loss = 0.0
            t0 = time.time()
            for xb, yb in dl_train:
                xb = xb.to(DEVICE, non_blocking=True); yb = yb.to(DEVICE, non_blocking=True)
                optimizer.zero_grad(set_to_none=True)
                with amp_ctx:
                    logits = model(xb)
                    B = logits.shape[0]
                    loss = criterion(logits.view(B*GRID*GRID, N_COLORS), yb.view(-1))
                # use scaler (or DummyScaler)
                scaled = scaler.scale(loss)
                # scaled might be loss tensor or a wrapper; scale() returns tensor in DummyScaler case
                scaled.backward()
                scaler.step(optimizer)
                scaler.update()
                running_loss += loss.item() * B
            running_loss /= len(dl_train.dataset)
            # validation
            model.eval()
            val_loss = 0.0; correct = 0; total = 0
            with torch.no_grad():
                for xb, yb in dl_val:
                    xb = xb.to(DEVICE, non_blocking=True); yb = yb.to(DEVICE, non_blocking=True)
                    with amp_ctx:
                        logits = model(xb)
                    B = logits.shape[0]
                    val_loss += criterion(logits.view(B*GRID*GRID, N_COLORS), yb.view(-1)).item() * B
                    preds = logits.argmax(dim=-1)
                    correct += (preds == yb).sum().item()
                    total += preds.numel()
            val_loss /= len(dl_val.dataset)
            elapsed = time.time()-t0
            acc = 100.0 * correct / total if total>0 else 0.0
            print(f"[Epoch {epoch}/{config['epochs']}] train_loss={running_loss:.4f} val_loss={val_loss:.4f} val_acc={acc:.2f}% time={elapsed:.1f}s")
            print_gpu_mem(f"After epoch {epoch}")
            if epoch % config["checkpoint_every"] == 0:
                ckpt = os.path.join(config["checkpoint_dir"], f"ckpt_epoch{epoch}.pt")
                torch.save({"epoch": epoch, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "config": config}, ckpt)
                print("Saved checkpoint:", ckpt)
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("CUDA OOM encountered. Memory diag:")
            print_gpu_mem("OOM")
            torch.cuda.empty_cache()
            if config.get("use_grad_accum", False):
                new_bs = max(1, config["batch_size"]//config["fallback_batch_divisor"])
                print(f"Fallback: batch {config['batch_size']} -> {new_bs}, using grad accumulation {config['grad_accum_steps']}")
                config2 = dict(config); config2["batch_size"] = new_bs
                _run_with_grad_accumulation(config2, model, optimizer, scaler, criterion, dl_train, dl_val)
            else:
                print("No fallback configured; reduce batch_size/latent_dim/hidden and retry.")
        else:
            raise e

def _run_with_grad_accumulation(config, model, optimizer, scaler, criterion, dl_train, dl_val):
    accum_steps = config.get("grad_accum_steps", 2)
    print("Running grad-accum fallback steps:", accum_steps)
    amp_ctx = autocast_cm()
    for epoch in range(1, config["epochs"]+1):
        model.train()
        t0 = time.time()
        running_loss = 0.0
        optimizer.zero_grad(set_to_none=True)
        for i, (xb, yb) in enumerate(dl_train):
            xb = xb.to(DEVICE, non_blocking=True); yb = yb.to(DEVICE, non_blocking=True)
            with amp_ctx:
                logits = model(xb); B = logits.shape[0]
                loss = criterion(logits.view(B*GRID*GRID, N_COLORS), yb.view(-1)) / accum_steps
            scaled = scaler.scale(loss); scaled.backward()
            if (i+1) % accum_steps == 0 or (i+1) == len(dl_train):
                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)
            running_loss += loss.item() * accum_steps * B
        running_loss /= len(dl_train.dataset)
        # validation like before
        model.eval()
        val_loss = 0.0; correct = 0; total = 0
        with torch.no_grad():
            for xb, yb in dl_val:
                xb = xb.to(DEVICE, non_blocking=True); yb = yb.to(DEVICE, non_blocking=True)
                with amp_ctx:
                    logits = model(xb)
                B = logits.shape[0]
                val_loss += criterion(logits.view(B*GRID*GRID, N_COLORS), yb.view(-1)).item() * B
                preds = logits.argmax(dim=-1)
                correct += (preds == yb).sum().item()
                total += preds.numel()
        val_loss /= len(dl_val.dataset)
        elapsed = time.time() - t0
        acc = 100.0 * correct / total if total > 0 else 0.0
        print(f"[GradAcc Epoch {epoch}/{config['epochs']}] train_loss={running_loss:.4f} val_loss={val_loss:.4f} val_acc={acc:.2f}% time={elapsed:.1f}s")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--n_examples", type=int, default=CONFIG["n_examples"])
    parser.add_argument("--batch_size", type=int, default=CONFIG["batch_size"])
    parser.add_argument("--latent_dim", type=int, default=CONFIG["latent_dim"])
    parser.add_argument("--epochs", type=int, default=CONFIG["epochs"])
    parser.add_argument("--num_workers", type=int, default=CONFIG["num_workers"])
    args = parser.parse_args()

    cfg = dict(CONFIG)
    cfg["n_examples"] = args.n_examples
    cfg["batch_size"] = args.batch_size
    cfg["latent_dim"] = args.latent_dim
    cfg["epochs"] = args.epochs
    cfg["num_workers"] = args.num_workers

    if cfg["latent_dim"] % cfg["ldm_rows"] != 0:
        print("Adjusting ldm_rows to divide latent_dim...")
        for r in range(min(cfg["latent_dim"], 64), 0, -1):
            if cfg["latent_dim"] % r == 0:
                cfg["ldm_rows"] = r; break

    print("CUDA available:", torch.cuda.is_available())
    if torch.cuda.is_available(): print("CUDA device:", torch.cuda.get_device_name(0))
    print("Starting CONFIG:", {k: cfg[k] for k in ("n_examples","latent_dim","encoder_hidden","decoder_hidden","ldm_rows","ldm_iters","batch_size","num_workers","epochs")})
    train(cfg)

if __name__ == "__main__":
    main()
