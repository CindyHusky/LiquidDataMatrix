#!/usr/bin/env python3
"""
arc_strict_grid_aio.py — ARC grid-to-grid test harness (strict)

- Runs only ARC-style grid-to-grid tasks (no arithmetic/boolean/spatial toy tasks).
- Dataset: small synthetic ARC-like transformations (invert_color, mirror_h, add_border, paint_center).
- Model: Encoder -> latent -> Decoder MLP (Pipeline).
- Loss: per-pixel CrossEntropyLoss (multi-class over N_COLORS).
- Reports per-pixel train/val loss and pixel-accuracy, baseline, label distribution.
- Adaptive weight decay optional.
- CLI + Tk GUI (click-to-run) with Start/Stop and logs.
- Quick self-test included for smoke-checks.

Usage:
  python arc_strict_grid_aio.py            # launches GUI (default)
  python arc_strict_grid_aio.py --mode cli --n_examples 240 --epochs 8 --batch_size 128
  python arc_strict_grid_aio.py --quick-test
"""
from __future__ import annotations

import argparse
import json
import math
import os
import random
import sys
import threading
import time
import traceback
from collections import Counter
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import queue
import tkinter as tk
from tkinter import ttk, scrolledtext

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# ------------------------
# Global settings & seeds
# ------------------------
SEED = 21
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def now() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S")

# ------------------------
# ARC-style synthetic dataset
# ------------------------
GRID = 6
N_COLORS = 6
TASKS = ['invert_color', 'mirror_h', 'add_border', 'paint_center']

def sample_task_instance(task: str, grid_size: int = GRID, n_colors: int = N_COLORS) -> Tuple[np.ndarray, np.ndarray]:
    """
    Creates one input grid (H,W) and target grid (H,W) for the selected ARC-style task.
    Each cell is an integer label in [0, n_colors-1].
    """
    x = np.zeros((grid_size, grid_size), dtype=np.int64)
    bg = random.randint(0, n_colors - 1)
    x.fill(bg)
    # add random rectangles/patches to make instances non-trivial
    for _ in range(random.randint(0, 2)):
        color = random.randint(0, n_colors - 1)
        r = random.randint(0, grid_size - 1)
        c = random.randint(0, grid_size - 1)
        h = random.randint(1, max(1, grid_size // 3))
        w = random.randint(1, max(1, grid_size // 3))
        r2 = min(grid_size, r + h)
        c2 = min(grid_size, c + w)
        x[r:r2, c:c2] = color

    y = x.copy()
    if task == 'invert_color':
        # swap two colors globally
        a, b = random.sample(range(n_colors), 2)
        mask_a = (x == a)
        mask_b = (x == b)
        y[mask_a] = b
        y[mask_b] = a
    elif task == 'mirror_h':
        y = np.fliplr(x)
    elif task == 'add_border':
        bc = random.randint(0, n_colors - 1)
        y[0, :] = bc
        y[-1, :] = bc
        y[:, 0] = bc
        y[:, -1] = bc
    elif task == 'paint_center':
        fc = random.randint(0, n_colors - 1)
        mid = grid_size // 2
        # paint a small center square depending on grid_size
        r0 = max(0, mid - 1)
        r1 = min(grid_size, mid + 1)
        c0 = max(0, mid - 1)
        c1 = min(grid_size, mid + 1)
        y[r0:r1, c0:c1] = fc
    else:
        raise ValueError(f"Unknown task: {task}")

    return x, y

def build_dataset(n_examples: int) -> Tuple[np.ndarray, np.ndarray, list]:
    """
    Build a dataset of n_examples. Returns (X, Y, T) where X,Y are numpy arrays:
      - X: (N, H, W) integer grids
      - Y: (N, H, W) integer grids (targets)
      - T: list of task names
    """
    X, Y, T = [], [], []
    for _ in range(n_examples):
        t = random.choice(TASKS)
        x, y = sample_task_instance(t)
        X.append(x)
        Y.append(y)
        T.append(t)
    X = np.stack(X).astype(np.int64)
    Y = np.stack(Y).astype(np.int64)
    return X, Y, T

def one_hot_grid(arr: np.ndarray, n_colors: int = N_COLORS) -> np.ndarray:
    """
    Convert integer grids (N,H,W) -> one-hot float arrays (N, H, W, N_COLORS).
    """
    arr = np.asarray(arr, dtype=np.int64)
    N, H, W = arr.shape
    # use advanced indexing for clarity
    return np.eye(n_colors, dtype=np.float32)[arr.reshape(-1)].reshape(N, H, W, n_colors)

class PrecomputedArcDataset(Dataset):
    """
    Dataset that returns (one-hot-input tensor, integer target tensor).
    Input: float32 (H, W, N_COLORS)
    Target: int64 (H, W)
    """
    def __init__(self, X_onehot: np.ndarray, Y: np.ndarray):
        assert X_onehot.shape[0] == Y.shape[0], "Mismatch between X and Y lengths"
        self.X = X_onehot
        self.Y = Y

    def __len__(self) -> int:
        return self.X.shape[0]

    def __getitem__(self, i: int):
        x = torch.from_numpy(self.X[i]).float()    # (H, W, N_COLORS)
        y = torch.from_numpy(self.Y[i]).long()     # (H, W)
        return x, y

# ------------------------
# Model: Encoder -> latent -> Decoder
# ------------------------
class EncoderMLP(nn.Module):
    def __init__(self, latent_dim: int = 256, hidden: int = 512):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(GRID * GRID * N_COLORS, hidden),
            nn.ReLU(inplace=True),
            nn.Linear(hidden, latent_dim),
            nn.ReLU(inplace=True),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x.view(x.size(0), -1))

class DecoderMLP(nn.Module):
    def __init__(self, latent_dim: int = 256, hidden: int = 512):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, hidden),
            nn.ReLU(inplace=True),
            nn.Linear(hidden, GRID * GRID * N_COLORS),
        )

    def forward(self, z: torch.Tensor) -> torch.Tensor:
        return self.net(z).view(z.size(0), GRID, GRID, N_COLORS)

class Pipeline(nn.Module):
    def __init__(self, latent_dim: int = 256, enc_h: int = 512, dec_h: int = 512):
        super().__init__()
        self.enc = EncoderMLP(latent_dim, enc_h)
        self.dec = DecoderMLP(latent_dim, dec_h)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.enc(x)
        logits = self.dec(z)
        return logits  # (B, H, W, N_COLORS)

def init_weights_xavier(module: nn.Module):
    for m in module.modules():
        if isinstance(m, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):
            if getattr(m, "weight", None) is not None:
                nn.init.xavier_uniform_(m.weight)
            if getattr(m, "bias", None) is not None:
                nn.init.zeros_(m.bias)

# ------------------------
# Adaptive Weight Decay (optional)
# ------------------------
class AdaptiveWeightDecay:
    def __init__(self, optimizer: optim.Optimizer, base_wd: float = 1e-5, ema_decay: float = 0.98,
                 resp: float = 1.0, wd_min: float = 1e-6, wd_max: float = 1e-2, eps: float = 1e-12):
        self.optimizer = optimizer
        self.base_wd = float(base_wd)
        self.ema_decay = float(ema_decay)
        self.resp = float(resp)
        self.wd_min = float(wd_min)
        self.wd_max = float(wd_max)
        self.eps = float(eps)
        self.ema = None
        self.baseline = None
        self.initialized = False

    def compute_grad_norm(self) -> float:
        total_sq = 0.0
        for group in self.optimizer.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                g = p.grad.data
                total_sq += float(g.float().pow(2).sum().item())
        return (total_sq ** 0.5) if total_sq > 0.0 else 0.0

    def step_update(self) -> Tuple[float, float]:
        grad_norm = self.compute_grad_norm()
        if not self.initialized:
            self.ema = grad_norm
            self.baseline = max(grad_norm, self.eps)
            self.initialized = True
        else:
            self.ema = self.ema_decay * self.ema + (1.0 - self.ema_decay) * grad_norm

        factor = (self.ema / (self.baseline + self.eps))
        new_wd = self.base_wd * (1.0 + self.resp * (factor - 1.0))
        new_wd = max(self.wd_min, min(self.wd_max, new_wd))

        for group in self.optimizer.param_groups:
            group['weight_decay'] = new_wd

        return grad_norm, new_wd

# ------------------------
# DataLoaders helper
# ------------------------
def prepare_dataloaders(X_all: np.ndarray, Y_all: np.ndarray, config: Dict[str, Any]):
    X_onehot = one_hot_grid(X_all, n_colors=N_COLORS)
    N = len(X_all)
    n_train = int(N * config.get("train_val_split", 0.8))
    # enforce at least 1 val example if possible (to avoid zero-size val)
    if N > 1 and (N - n_train) == 0:
        n_train = max(1, n_train - 1)

    train_ds = PrecomputedArcDataset(X_onehot[:n_train], Y_all[:n_train])
    val_ds = PrecomputedArcDataset(X_onehot[n_train:], Y_all[n_train:])

    pin_memory = bool(config.get("pin_memory", False)) and torch.cuda.is_available()
    requested_workers = int(config.get("num_workers", 0) or 0)
    try:
        max_workers = max(0, min(requested_workers, os.cpu_count() or 0))
    except Exception:
        max_workers = 0

    dl_train = DataLoader(train_ds,
                          batch_size=config.get("batch_size", 32),
                          shuffle=True,
                          num_workers=max_workers,
                          pin_memory=pin_memory)

    dl_val = DataLoader(val_ds,
                        batch_size=config.get("batch_size", 32),
                        shuffle=False,
                        num_workers=max(0, max_workers // 2),
                        pin_memory=pin_memory)

    return dl_train, dl_val, n_train

def compute_majority_baseline_from_array(Y_all: np.ndarray, n_train: int) -> float:
    y_val = Y_all[n_train:].reshape(-1)
    if y_val.size == 0:
        return 0.0
    cnt = Counter(y_val.tolist())
    mode_count = cnt.most_common(1)[0][1]
    return 100.0 * mode_count / y_val.size

def label_distribution_from_array(Y_all: np.ndarray, n_train: int, n_colors: int = N_COLORS) -> np.ndarray:
    y_val = Y_all[n_train:].reshape(-1)
    counts = np.zeros(n_colors, dtype=np.int64)
    if y_val.size == 0:
        return counts
    vals, cts = np.unique(y_val, return_counts=True)
    for v, c in zip(vals, cts):
        counts[int(v)] = int(c)
    return counts

# ------------------------
# Core: run ARC grid tests (strict)
# ------------------------
DEFAULT_CFG = {
    "n_examples": 240,
    "train_val_split": 0.8,
    "latent_dim": 256,
    "encoder_hidden": 512,
    "decoder_hidden": 512,
    "epochs": 8,
    "batch_size": 64,
    "lr": 1e-3,
    "weight_decay": 1e-5,
    "num_workers": 2,
    "pin_memory": True,
    "checkpoint_dir": "checkpoints_arc",
    "adaptive_wd": True,
    "wd_ema_decay": 0.98,
    "wd_resp": 1.0,
    "wd_min": 1e-6,
    "wd_max": 1e-2,
}

def run_arc_tests(cfg: Dict[str, Any], emit=print, stop_event: Optional[threading.Event] = None) -> Dict[str, Any]:
    """
    Runs strict ARC grid-to-grid training using Pipeline (encoder->decoder).
    Returns a summary dict.
    """
    if stop_event is None:
        stop_event = threading.Event()

    os.makedirs(cfg.get("checkpoint_dir", "checkpoints_arc"), exist_ok=True)
    emit("Building ARC dataset...")
    X_all, Y_all, T_all = build_dataset(cfg.get("n_examples", DEFAULT_CFG["n_examples"]))
    dl_train, dl_val, n_train = prepare_dataloaders(X_all, Y_all, cfg)
    emit(f"Train size: {len(dl_train.dataset)} Val size: {len(dl_val.dataset)}")
    emit(f"Device: {DEVICE}")

    baseline = compute_majority_baseline_from_array(Y_all, n_train)
    emit(f"Majority-class baseline (val) = {baseline:.2f}%")
    distr = label_distribution_from_array(Y_all, n_train, N_COLORS)
    emit("Validation label distribution (per-class counts): " + ", ".join(str(int(x)) for x in distr.tolist()))

    # model, optimizer, criterion
    model = Pipeline(cfg.get("latent_dim", 256), cfg.get("encoder_hidden", 512), cfg.get("decoder_hidden", 512)).to(DEVICE)
    init_weights_xavier(model)

    optimizer = optim.AdamW(model.parameters(), lr=cfg.get("lr", 1e-3), weight_decay=cfg.get("weight_decay", 1e-5))
    criterion = nn.CrossEntropyLoss()

    awd = None
    if cfg.get("adaptive_wd", False):
        awd = AdaptiveWeightDecay(
            optimizer,
            base_wd=cfg.get("weight_decay", 1e-5),
            ema_decay=cfg.get("wd_ema_decay", 0.98),
            resp=cfg.get("wd_resp", 1.0),
            wd_min=cfg.get("wd_min", 1e-6),
            wd_max=cfg.get("wd_max", 1e-2),
        )

    GRID_PIXELS = GRID * GRID
    total_examples_processed = 0
    start_time = time.time()
    total_steps = 0
    for epoch in range(1, cfg.get("epochs", 8) + 1):
        if stop_event.is_set():
            emit(f"Stop requested before epoch {epoch}.")
            break
        model.train()
        total_train_pixel_loss = 0.0
        epoch_wd_accum = 0.0
        wd_steps = 0
        t0 = time.time()
        steps_this_epoch = 0

        for xb, yb in dl_train:
            if stop_event.is_set():
                break
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)  # xb: (B,H,W,C), yb: (B,H,W)
            optimizer.zero_grad()
            logits = model(xb)  # (B,H,W,C)
            B = logits.shape[0]
            loss = criterion(logits.view(B * GRID_PIXELS, N_COLORS), yb.view(-1))
            loss.backward()
            if awd is not None:
                grad_norm, cur_wd = awd.step_update()
                epoch_wd_accum += cur_wd
                wd_steps += 1
            optimizer.step()
            total_train_pixel_loss += loss.item() * (B * GRID_PIXELS)

            total_examples_processed += (B * GRID_PIXELS)
            steps_this_epoch += 1
            total_steps += 1

        train_loss_per_pixel = total_train_pixel_loss / max(1, len(dl_train.dataset) * GRID_PIXELS)

        # validation
        model.eval()
        total_val_pixel_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for xb, yb in dl_val:
                if stop_event.is_set():
                    break
                xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                B = xb.shape[0]
                logits = model(xb)
                vloss = criterion(logits.view(B * GRID_PIXELS, N_COLORS), yb.view(-1))
                total_val_pixel_loss += vloss.item() * (B * GRID_PIXELS)
                preds = logits.argmax(dim=-1)
                correct += (preds == yb).sum().item()
                total += int(preds.numel())

        # avoid division by zero if val set is empty (shouldn't happen but guard)
        val_loss_per_pixel = total_val_pixel_loss / max(1, len(dl_val.dataset) * GRID_PIXELS)
        acc = 100.0 * correct / total if total > 0 else 0.0
        elapsed = time.time() - t0
        avg_wd = (epoch_wd_accum / wd_steps) if wd_steps > 0 else optimizer.param_groups[0].get('weight_decay', cfg.get("weight_decay"))

        # throughput metrics
        epoch_examples = len(dl_train.dataset) * GRID_PIXELS
        examples_per_sec = epoch_examples / elapsed if elapsed > 0 else float("inf")
        steps_per_sec = (steps_this_epoch / elapsed) if elapsed > 0 else float("inf")

        emit(f"[Epoch {epoch}/{cfg.get('epochs')}] train_loss(pix)={train_loss_per_pixel:.6f} val_loss(pix)={val_loss_per_pixel:.6f} val_acc={acc:.2f}% time={elapsed:.4f}s wd={avg_wd:.6g} examples/s={examples_per_sec:.2f} steps/s={steps_per_sec:.2f}")

    duration = time.time() - start_time
    emit("ARC grid-to-grid training finished.")
    summary = {
        'run_started_at': now(),
        'duration_seconds': float(duration),
        'device': str(DEVICE),
        'config': cfg,
        'total_examples_processed': int(total_examples_processed),
        'total_steps': int(total_steps),
    }
    return summary

# ------------------------
# Quick self-test (ARC-only)
# ------------------------
def quick_self_test_arc(emit=print):
    emit("Running quick self-test (strict ARC)...")
    try:
        cfg = dict(DEFAULT_CFG)
        cfg.update({
            "n_examples": 64,
            "batch_size": 8,
            "epochs": 2,
            "latent_dim": 64,
            "encoder_hidden": 64,
            "decoder_hidden": 64,
            "adaptive_wd": False,
            "num_workers": 0,
            "pin_memory": False,
        })
        stop_event = threading.Event()
        summary = run_arc_tests(cfg, emit=emit, stop_event=stop_event)
        emit("Quick ARC summary: " + json.dumps(summary, indent=2))
        emit("Self-test completed successfully.")
    except Exception as e:
        emit("Self-test failed with exception:")
        emit(repr(e))
        traceback.print_exc()

# ------------------------
# GUI (Tkinter) — streamlined for ARC-only runs
# ------------------------
def launch_tk_gui():
    root = tk.Tk()
    root.title("ARC Strict Grid AIO — Click & Run")
    root.geometry("760x540")

    frm = ttk.Frame(root, padding=10)
    frm.pack(fill="both", expand=True)

    cfg_frame = ttk.LabelFrame(frm, text="Run Configuration", padding=8)
    cfg_frame.pack(fill="x")

    ttk.Label(cfg_frame, text="n_examples:").grid(row=0, column=0, sticky="w", padx=4, pady=4)
    n_examples_var = tk.IntVar(value=DEFAULT_CFG["n_examples"])
    ttk.Entry(cfg_frame, textvariable=n_examples_var, width=10).grid(row=0, column=1, sticky="w")

    ttk.Label(cfg_frame, text="Batch size:").grid(row=0, column=2, sticky="w", padx=4, pady=4)
    batch_var = tk.IntVar(value=DEFAULT_CFG["batch_size"])
    ttk.Entry(cfg_frame, textvariable=batch_var, width=8).grid(row=0, column=3, sticky="w")

    ttk.Label(cfg_frame, text="Epochs:").grid(row=1, column=0, sticky="w", padx=4, pady=4)
    epochs_var = tk.IntVar(value=DEFAULT_CFG["epochs"])
    ttk.Entry(cfg_frame, textvariable=epochs_var, width=8).grid(row=1, column=1, sticky="w")

    ttk.Label(cfg_frame, text="latent_dim:").grid(row=1, column=2, sticky="w", padx=4, pady=4)
    latent_var = tk.IntVar(value=DEFAULT_CFG["latent_dim"])
    ttk.Entry(cfg_frame, textvariable=latent_var, width=8).grid(row=1, column=3, sticky="w")

    adaptive_var = tk.BooleanVar(value=DEFAULT_CFG["adaptive_wd"])
    ttk.Checkbutton(cfg_frame, text="adaptive_wd", variable=adaptive_var).grid(row=2, column=0, padx=4, pady=4, sticky="w")

    # Logs
    log_frame = ttk.LabelFrame(frm, text="Logs", padding=8)
    log_frame.pack(fill="both", expand=True, pady=(8, 0))
    log_box = scrolledtext.ScrolledText(log_frame, state="disabled", wrap="word")
    log_box.pack(fill="both", expand=True)

    # Buttons
    btn_frame = ttk.Frame(frm)
    btn_frame.pack(fill="x", pady=8)
    start_btn = ttk.Button(btn_frame, text="Start")
    start_btn.pack(side="left")
    stop_btn = ttk.Button(btn_frame, text="Stop", state="disabled")
    stop_btn.pack(side="left", padx=6)
    quick_btn = ttk.Button(btn_frame, text="Quick Self-Test")
    quick_btn.pack(side="left", padx=6)

    q = queue.Queue()
    stop_event = threading.Event()
    worker_thread = None

    def emit_to_queue(msg: str):
        q.put(msg)

    def poll_queue():
        try:
            while True:
                msg = q.get_nowait()
                log_box.configure(state="normal")
                log_box.insert("end", f"{now()} | {msg}\n")
                log_box.see("end")
                log_box.configure(state="disabled")
        except queue.Empty:
            pass
        root.after(200, poll_queue)

    def make_cfg():
        return {
            "n_examples": int(n_examples_var.get()),
            "batch_size": int(batch_var.get()),
            "epochs": int(epochs_var.get()),
            "latent_dim": int(latent_var.get()),
            "encoder_hidden": int(max(64, latent_var.get())),
            "decoder_hidden": int(max(64, latent_var.get())),
            "lr": 1e-3,
            "weight_decay": 1e-5,
            "num_workers": 0,
            "pin_memory": True,
            "checkpoint_dir": "checkpoints_arc",
            "adaptive_wd": bool(adaptive_var.get()),
            "wd_ema_decay": 0.98,
            "wd_resp": 1.0,
            "wd_min": 1e-6,
            "wd_max": 1e-2,
            "train_val_split": 0.8,
        }

    def worker_fn(cfg_local):
        nonlocal worker_thread, stop_event
        try:
            emit_to_queue("Starting ARC strict run...")
            summary = run_arc_tests(cfg_local, emit=emit_to_queue, stop_event=stop_event)
            emit_to_queue("ARC run finished — summary: " + json.dumps(summary))
        except Exception as e:
            emit_to_queue("ARC failure: " + repr(e))
            traceback.print_exc()
        finally:
            start_btn.config(state="normal")
            stop_btn.config(state="disabled")

    def on_start():
        nonlocal worker_thread, stop_event
        start_btn.config(state="disabled")
        stop_btn.config(state="normal")
        stop_event.clear()
        cfg_local = make_cfg()
        worker_thread = threading.Thread(target=worker_fn, args=(cfg_local,), daemon=True)
        worker_thread.start()

    def on_stop():
        stop_event.set()
        emit_to_queue("Stop requested — attempting graceful interruption...")

    def on_quick_test():
        def _qt():
            try:
                emit_to_queue("Starting quick self-test (ARC-only)...")
                quick_self_test_arc(emit=emit_to_queue)
                emit_to_queue("Quick self-test finished.")
            except Exception as e:
                emit_to_queue("Quick self-test failed: " + repr(e))
                traceback.print_exc()
        threading.Thread(target=_qt, daemon=True).start()

    start_btn.config(command=on_start)
    stop_btn.config(command=on_stop)
    quick_btn.config(command=on_quick_test)

    root.after(200, poll_queue)
    root.mainloop()

# ------------------------
# CLI
# ------------------------
def main_cli():
    parser = argparse.ArgumentParser(description="ARC Strict Grid AIO — runs only ARC grid-to-grid tests")
    parser.add_argument("--mode", choices=["gui", "cli"], default="gui", help="Run mode (gui or cli)")
    parser.add_argument("--quick-test", action="store_true", help="Run a quick self-test (smoke-check)")
    parser.add_argument("--n_examples", type=int, default=DEFAULT_CFG["n_examples"])
    parser.add_argument("--batch_size", type=int, default=DEFAULT_CFG["batch_size"])
    parser.add_argument("--epochs", type=int, default=DEFAULT_CFG["epochs"])
    parser.add_argument("--latent_dim", type=int, default=DEFAULT_CFG["latent_dim"])
    parser.add_argument("--adaptive_wd", action="store_true", help="Enable adaptive weight decay")
    args = parser.parse_args()

    if args.mode == "gui":
        try:
            launch_tk_gui()
            return
        except Exception as e:
            print("GUI failed to launch:", e)
            traceback.print_exc()
            # fall through to CLI

    if args.quick_test:
        quick_self_test_arc(emit=print)
        return

    # CLI strict ARC run
    cfg = dict(DEFAULT_CFG)
    cfg["n_examples"] = args.n_examples
    cfg["batch_size"] = args.batch_size
    cfg["epochs"] = args.epochs
    cfg["latent_dim"] = args.latent_dim
    cfg["encoder_hidden"] = max(64, args.latent_dim)
    cfg["decoder_hidden"] = max(64, args.latent_dim)
    cfg["adaptive_wd"] = bool(args.adaptive_wd)

    print("CUDA available:", torch.cuda.is_available())
    if torch.cuda.is_available():
        try:
            print("CUDA device:", torch.cuda.get_device_name(0))
        except Exception:
            pass

    print("Starting strict ARC run with config:", {k: cfg[k] for k in ("n_examples", "latent_dim", "encoder_hidden", "decoder_hidden", "batch_size", "epochs")})
    try:
        summary = run_arc_tests(cfg, emit=print, stop_event=threading.Event())
        print("Run summary:")
        print(json.dumps(summary, indent=2))
    except KeyboardInterrupt:
        print("Run interrupted by user (KeyboardInterrupt).")
    except Exception as e:
        print("Run failed:", repr(e))
        traceback.print_exc()

if __name__ == "__main__":
    main_cli()
