#!/usr/bin/env python3

"""
arc_strict_grid_aio.py â€” ARC grid-to-grid test harness (strict)

- Runs only ARC-style grid-to-grid tasks (no arithmetic/boolean/spatial toy tasks).
- Dataset: small synthetic ARC-like transformations (invert_color, mirror_h, add_border, paint_center).
- Model: Encoder -> latent -> Decoder MLP (Pipeline).
- Loss: per-pixel CrossEntropyLoss or soft-target loss (for MixUp/CutMix).
- Reports per-pixel train/val loss and pixel-accuracy, baseline, label distribution.
- Adaptive weight decay optional.
- CLI + Tk GUI (click-to-run) with Start/Stop and logs.
- Quick self-test included for smoke-checks.

Usage:
  python arc_strict_grid_aio.py            # launches GUI (default)
  python arc_strict_grid_aio.py --mode cli --n_examples 240 --epochs 8 --batch_size 128
  python arc_strict_grid_aio.py --quick-test
"""
from __future__ import annotations

import argparse
import json
import math
import os
import random
import sys
import threading
import time
import traceback
from collections import Counter
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import queue
import tkinter as tk
from tkinter import ttk, scrolledtext

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import Dataset, DataLoader

# ------------------------
# Global settings & seeds
# ------------------------
SEED = 21
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# performance helpers (safe defaults)
torch.backends.cudnn.benchmark = True

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def now() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S")


import random
from typing import Tuple, List
import numpy as np
import torch
from torch.utils.data import Dataset

# ------------------------
# ARC-style synthetic dataset
# ------------------------
GRID = 6
N_COLORS = 6
TASKS = ['invert_color', 'mirror_h', 'add_border', 'paint_center']


def sample_task_instance(task: str, grid_size: int = GRID, n_colors: int = N_COLORS) -> Tuple[np.ndarray, np.ndarray]:
    x = np.zeros((grid_size, grid_size), dtype=np.int64)
    bg = random.randint(0, n_colors - 1)
    x.fill(bg)

    # add random rectangles/patches
    for _ in range(random.randint(0, 2)):
        color = random.randint(0, n_colors - 1)
        r = random.randint(0, grid_size - 1)
        c = random.randint(0, grid_size - 1)
        h = random.randint(1, max(1, grid_size // 3))
        w = random.randint(1, max(1, grid_size // 3))
        r2 = min(grid_size, r + h)
        c2 = min(grid_size, c + w)
        x[r:r2, c:c2] = color

    y = x.copy()
    if task == 'invert_color':
        # swap two distinct colors globally
        a, b = random.sample(range(n_colors), 2)
        mask_a = (x == a)
        mask_b = (x == b)
        y[mask_a] = b
        y[mask_b] = a
    elif task == 'mirror_h':
        y = np.fliplr(x).copy()
    elif task == 'add_border':
        bc = random.randint(0, n_colors - 1)
        y[0, :] = bc
        y[-1, :] = bc
        y[:, 0] = bc
        y[:, -1] = bc
    elif task == 'paint_center':
        fc = random.randint(0, n_colors - 1)
        mid = grid_size // 2
        r0 = max(0, mid - 1)
        r1 = min(grid_size, mid + 1)
        c0 = max(0, mid - 1)
        c1 = min(grid_size, mid + 1)
        y[r0:r1, c0:c1] = fc
    else:
        raise ValueError(f"Unknown task: {task}")

    return x, y


def build_dataset(n_examples: int) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    X, Y, T = [], [], []
    for _ in range(n_examples):
        t = random.choice(TASKS)
        x, y = sample_task_instance(t)
        X.append(x)
        Y.append(y)
        T.append(t)
    X = np.stack(X).astype(np.int64)
    Y = np.stack(Y).astype(np.int64)
    return X, Y, T


def one_hot_grid(arr: np.ndarray, n_colors: int = N_COLORS) -> np.ndarray:
    """
    Convert integer grids (N,H,W) -> one-hot float arrays in channel-first format (N, C, H, W).
    Accepts input shape (H,W) single-grid as well.
    """
    arr = np.asarray(arr, dtype=np.int64)
    if arr.ndim == 2:
        arr = arr[np.newaxis, ...]
    if arr.ndim != 3:
        raise ValueError("one_hot_grid expects array of shape (N,H,W) or (H,W)")
    N, H, W = arr.shape
    flat = arr.reshape(-1)
    if flat.max() >= n_colors or flat.min() < 0:
        raise ValueError("one_hot_grid: values out of range [0, n_colors-1]")
    eye = np.eye(n_colors, dtype=np.float32)   # (C,C)
    onehot = eye[flat].reshape(N, H, W, n_colors)  # (N,H,W,C)
    # convert to channel-first for PyTorch: (N, C, H, W)
    onehot = onehot.transpose(0, 3, 1, 2).copy()
    return onehot


class PrecomputedArcDataset(Dataset):
    """
    Dataset that returns (input tensor, target tensor).
    Input: float32 (C, H, W) channel-first one-hot
    Target: int64 (H, W)
    """

    def __init__(self, X_onehot: np.ndarray, Y: np.ndarray):
        assert X_onehot.shape[0] == Y.shape[0], "Mismatch between X and Y lengths"
        # Expect X_onehot already channel-first (N, C, H, W)
        self.X = X_onehot
        self.Y = Y

    def __len__(self) -> int:
        return self.X.shape[0]

    def __getitem__(self, i: int):
        x = torch.from_numpy(self.X[i]).float()    # (C, H, W)
        y = torch.from_numpy(self.Y[i]).long()     # (H, W)
        return x, y

# ------------------------
# Model: Encoder -> latent -> Decoder
# ------------------------
import torch
import torch.nn as nn
from typing import Optional

# module defaults (you can override per-class via constructor args)
DEFAULT_GRID = 6
DEFAULT_N_COLORS = 6


class EncoderMLP(nn.Module):
    """
    Encoder MLP.

    - expects input x of shape (B, C, H, W) or (B, H, W, C) where C == n_colors
    - returns latent z of shape (B, latent_dim)
    """
    def __init__(
        self,
        latent_dim: int = 256,
        hidden: int = 512,
        grid: int = DEFAULT_GRID,
        n_colors: int = DEFAULT_N_COLORS,
    ):
        super().__init__()
        self.grid = int(grid)
        self.n_colors = int(n_colors)
        input_dim = self.grid * self.grid * self.n_colors
        # Flatten() will flatten from dim=1 -> (B, input_dim)
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(input_dim, hidden),
            nn.ReLU(inplace=True),
            nn.Linear(hidden, latent_dim),
        )

    def _ensure_ch_first(self, x: torch.Tensor) -> torch.Tensor:
        # Accept NCHW or NHWC (robustness)
        if x.dim() != 4:
            raise ValueError(f"EncoderMLP expects a 4D tensor (B, C, H, W) or (B, H, W, C), got shape {tuple(x.shape)}")
        # if already channel-first
        if x.shape[1] == self.n_colors:
            return x
        # if channel-last NHWC
        if x.shape[-1] == self.n_colors:
            return x.permute(0, 3, 1, 2).contiguous()
        raise ValueError(f"Input channel mismatch: expected {self.n_colors} channels but got shapes "
                         f"C={x.shape[1]} or last={x.shape[-1]}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self._ensure_ch_first(x)
        return self.net(x)


class DecoderMLP(nn.Module):
    """
    Decoder MLP
    - expects latent z (B, latent_dim)
    - returns logits (B, C, H, W) suitable for nn.CrossEntropyLoss with targets (B, H, W)
    """
    def __init__(
        self,
        latent_dim: int = 256,
        hidden: int = 512,
        grid: int = DEFAULT_GRID,
        n_colors: int = DEFAULT_N_COLORS,
    ):
        super().__init__()
        self.grid = int(grid)
        self.n_colors = int(n_colors)
        output_dim = self.grid * self.grid * self.n_colors
        self.net = nn.Sequential(
            nn.Linear(latent_dim, hidden),
            nn.ReLU(inplace=True),
            nn.Linear(hidden, output_dim),
        )

    def forward(self, z: torch.Tensor) -> torch.Tensor:
        if z.dim() != 2:
            raise ValueError(f"DecoderMLP expects a 2D latent (B, latent_dim), got shape {tuple(z.shape)}")
        b = z.size(0)
        out = self.net(z)                        # (B, output_dim)
        out = out.view(b, self.n_colors, self.grid, self.grid)  # (B, C, H, W)
        return out


class Pipeline(nn.Module):
    """
    Simple encoder->decoder pipeline returning logits (B, C, H, W).
    """
    def __init__(
        self,
        latent_dim: int = 256,
        enc_h: int = 512,
        dec_h: int = 512,
        grid: int = DEFAULT_GRID,
        n_colors: int = DEFAULT_N_COLORS,
    ):
        super().__init__()
        self.grid = int(grid)
        self.n_colors = int(n_colors)
        self.enc = EncoderMLP(latent_dim=latent_dim, hidden=enc_h, grid=self.grid, n_colors=self.n_colors)
        self.dec = DecoderMLP(latent_dim=latent_dim, hidden=dec_h, grid=self.grid, n_colors=self.n_colors)

    def _ensure_ch_first(self, x: torch.Tensor) -> torch.Tensor:
        # same logic as EncoderMLP
        if x.dim() != 4:
            raise ValueError(f"Pipeline expects a 4D tensor (B, C, H, W) or (B, H, W, C), got shape {tuple(x.shape)}")
        if x.shape[1] == self.n_colors:
            return x
        if x.shape[-1] == self.n_colors:
            return x.permute(0, 3, 1, 2).contiguous()
        raise ValueError(f"Input channel mismatch: expected {self.n_colors} channels but got shapes "
                         f"C={x.shape[1]} or last={x.shape[-1]}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self._ensure_ch_first(x)
        z = self.enc(x)      # (B, latent_dim)
        logits = self.dec(z) # (B, C, H, W)
        return logits


def init_weights_xavier(module: nn.Module):
    """
    Apply Xavier uniform init for Linear/Conv layers and zero biases.
    Usage: model.apply(init_weights_xavier)
    """
    if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):
        if getattr(module, "weight", None) is not None:
            nn.init.xavier_uniform_(module.weight)
        if getattr(module, "bias", None) is not None:
            nn.init.zeros_(module.bias)


if __name__ == "__main__":
    # Quick model test
    GRID = 6
    N_COLORS = 6
    BATCH = 4
    latent_dim = 128

    model = Pipeline(latent_dim=latent_dim, enc_h=256, dec_h=256, grid=GRID, n_colors=N_COLORS)
    model.apply(init_weights_xavier)

    # Test with channel-first (B, C, H, W)
    x_nchw = torch.randn(BATCH, N_COLORS, GRID, GRID)
    out1 = model(x_nchw)
    assert out1.shape == (BATCH, N_COLORS, GRID, GRID)
    print("NCHW pass:", out1.shape)

    # Test with channel-last (B, H, W, C)
    x_nhwc = torch.randn(BATCH, GRID, GRID, N_COLORS)
    out2 = model(x_nhwc)
    assert out2.shape == (BATCH, N_COLORS, GRID, GRID)
    print("NHWC pass:", out2.shape)

    # Ensure CrossEntropy compatibility: targets shape (B, H, W) of ints in [0, N_COLORS)
    targets = torch.randint(0, N_COLORS, (BATCH, GRID, GRID), dtype=torch.long)
    loss_fn = nn.CrossEntropyLoss()  # expects logits (B, C, H, W) and targets (B, H, W)
    loss = loss_fn(out1, targets)
    print("Loss computed:", float(loss))
    print("Model test completed successfully.")

# ------------------------
# Adaptive Weight Decay (optional)
# ------------------------
class AdaptiveWeightDecay:
    def __init__(self, optimizer: optim.Optimizer, base_wd: float = 1e-5, ema_decay: float = 0.98,
                 resp: float = 1.0, wd_min: float = 1e-6, wd_max: float = 1e-2, eps: float = 1e-12):
        self.optimizer = optimizer
        self.base_wd = float(base_wd)
        self.ema_decay = float(ema_decay)
        self.resp = float(resp)
        self.wd_min = float(wd_min)
        self.wd_max = float(wd_max)
        self.eps = float(eps)
        self.ema = None
        self.baseline = None
        self.initialized = False

    def compute_grad_norm(self) -> float:
        total_sq = 0.0
        for group in self.optimizer.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                g = p.grad.data
                total_sq += float(g.float().pow(2).sum().item())
        return (total_sq ** 0.5) if total_sq > 0.0 else 0.0

    def step_update(self) -> Tuple[float, float]:
        grad_norm = self.compute_grad_norm()
        if not self.initialized:
            self.ema = grad_norm
            self.baseline = max(grad_norm, self.eps)
            self.initialized = True
        else:
            self.ema = self.ema_decay * self.ema + (1.0 - self.ema_decay) * grad_norm

        factor = (self.ema / (self.baseline + self.eps))
        new_wd = self.base_wd * (1.0 + self.resp * (factor - 1.0))
        new_wd = max(self.wd_min, min(self.wd_max, new_wd))

        for group in self.optimizer.param_groups:
            group['weight_decay'] = new_wd

        return grad_norm, new_wd


# ------------------------
# MixUp / CutMix helpers (operates on tensors shaped (B,H,W,C) one-hot channels last)
# ------------------------
def rand_beta(alpha: float) -> float:
    if alpha <= 0.0:
        return 1.0
    return float(np.random.beta(alpha, alpha))


def mixup_cutmix_batch(x: torch.Tensor, y: torch.Tensor, mixup_alpha: float = 0.2, cutmix_alpha: float = 1.0,
                       prob: float = 0.5, device: torch.device = DEVICE):
    """
    x: (B, H, W, C) float
    y: (B, H, W) long
    Returns:
      mixed_x: (B, H, W, C) float (on same device as x)
      mixed_y_soft: (B*H*W, C) float soft labels (on device)
    """
    B, H, W, C = x.shape
    x = x.to(device)
    y = y.to(device)

    if B == 1 or random.random() >= prob:
        flat_targets = F.one_hot(y.view(-1), num_classes=C).float().to(device)
        return x, flat_targets

    # permutation indices
    perm = torch.randperm(B, device=device)
    x2 = x[perm]
    y2 = y[perm]

    if random.random() < 0.5:
        # MixUp
        lam = rand_beta(mixup_alpha)
        lam_t = torch.tensor(lam, device=device, dtype=torch.float32)
        mixed_x = lam_t * x + (1.0 - lam_t) * x2
        t1 = F.one_hot(y.view(-1), num_classes=C).float().to(device)
        t2 = F.one_hot(y2.view(-1), num_classes=C).float().to(device)
        mixed_y = lam_t * t1 + (1.0 - lam_t) * t2
        return mixed_x, mixed_y
    else:
        # CutMix on grid
        lam = rand_beta(cutmix_alpha)
        cut_ratio = math.sqrt(max(1.0 - lam, 0.0))
        cut_h = max(1, int(round(H * cut_ratio)))
        cut_w = max(1, int(round(W * cut_ratio)))

        cy = random.randint(0, H - 1)
        cx = random.randint(0, W - 1)
        y1 = max(0, cy - cut_h // 2)
        y2_ = min(H, cy + (cut_h - cut_h // 2))
        x1 = max(0, cx - cut_w // 2)
        x2_ = min(W, cx + (cut_w - cut_w // 2))

        mixed_x = x.clone()
        mixed_x[:, y1:y2_, x1:x2_, :] = x2[:, y1:y2_, x1:x2_, :]

        patched_area = float((y2_ - y1) * (x2_ - x1))
        lam_area = 1.0 - (patched_area / float(H * W))
        t1 = F.one_hot(y.view(-1), num_classes=C).float().to(device)
        t2 = F.one_hot(y2.view(-1), num_classes=C).float().to(device)
        mixed_y = lam_area * t1 + (1.0 - lam_area) * t2
        return mixed_x, mixed_y


# ------------------------
# DataLoaders helper
# ------------------------
def prepare_dataloaders(X_all: np.ndarray, Y_all: np.ndarray, config: Dict[str, Any]):
    X_onehot = one_hot_grid(X_all, n_colors=N_COLORS)
    N = len(X_all)
    n_train = int(N * config.get("train_val_split", 0.8))
    # enforce at least 1 val example if possible (to avoid zero-size val)
    if N > 1 and (N - n_train) == 0:
        n_train = max(1, n_train - 1)

    train_ds = PrecomputedArcDataset(X_onehot[:n_train], Y_all[:n_train])
    val_ds = PrecomputedArcDataset(X_onehot[n_train:], Y_all[n_train:])

    pin_memory = bool(config.get("pin_memory", False)) and torch.cuda.is_available()
    requested_workers = int(config.get("num_workers", 0) or 0)
    try:
        max_workers = max(0, min(requested_workers, os.cpu_count() or 0))
    except Exception:
        max_workers = 0

    dl_train = DataLoader(train_ds,
                          batch_size=config.get("batch_size", 32),
                          shuffle=True,
                          num_workers=max_workers,
                          pin_memory=pin_memory)

    dl_val = DataLoader(val_ds,
                        batch_size=config.get("batch_size", 32),
                        shuffle=False,
                        num_workers=max(0, max_workers // 2),
                        pin_memory=pin_memory)

    return dl_train, dl_val, n_train


def compute_majority_baseline_from_array(Y_all: np.ndarray, n_train: int) -> float:
    y_val = Y_all[n_train:].reshape(-1)
    if y_val.size == 0:
        return 0.0
    cnt = Counter(y_val.tolist())
    mode_count = cnt.most_common(1)[0][1]
    return 100.0 * mode_count / y_val.size


def label_distribution_from_array(Y_all: np.ndarray, n_train: int, n_colors: int = N_COLORS) -> np.ndarray:
    y_val = Y_all[n_train:].reshape(-1)
    counts = np.zeros(n_colors, dtype=np.int64)
    if y_val.size == 0:
        return counts
    vals, cts = np.unique(y_val, return_counts=True)
    for v, c in zip(vals, cts):
        counts[int(v)] = int(c)
    return counts


# ------------------------
# Core: run ARC grid tests (strict)
# ------------------------
import os
import time
import threading
from typing import Any, Dict, Optional
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingLR


DEFAULT_CFG = {
    "n_examples": 240,
    "train_val_split": 0.8,
    "latent_dim": 256,
    "encoder_hidden": 512,
    "decoder_hidden": 512,
    "epochs": 8,
    "batch_size": 64,
    "lr": 3e-4,
    "weight_decay": 1e-3,
    "num_workers": 2,
    "pin_memory": True,
    "checkpoint_dir": "checkpoints_arc",
    "adaptive_wd": True,
    "wd_ema_decay": 0.98,
    "wd_resp": 1.0,
    "wd_min": 1e-6,
    "wd_max": 1e-2,
    "mixup_alpha": 0.2,
    "cutmix_alpha": 1.0,
    "mix_prob": 0.5,
    "label_smoothing": None,  # or 0.05 to enable CE label smoothing fallback
    "save_every_n_epochs": 0,
    "seed": None,  # optional: set int for deterministic runs
}

# ---------------------------
# Utility helpers (light)
# ---------------------------
def _ensure_required(name: str):
    raise RuntimeError(f"Required object/function '{name}' is not defined in the environment. "
                       "Make sure it's imported or defined before calling run_arc_tests().")

def _to_onehot(labels: torch.Tensor, n_classes: int, device=None, dtype=torch.float32):
    # labels: (B,H,W) -> returns (B*H*W, n_classes)
    flat = labels.view(-1).long()
    onehot = torch.zeros((flat.shape[0], n_classes), device=device, dtype=dtype)
    onehot.scatter_(1, flat.unsqueeze(1), 1.0)
    return onehot

# ---------------------------
# Main upgraded function
# ---------------------------
def run_arc_tests(cfg: Dict[str, Any], emit=print, stop_event: Optional[threading.Event] = None) -> Dict[str, Any]:
    """
    Upgraded/robust run_arc_tests wrapper.
    Expects the following names to be available in the module globals:
      - build_dataset(n_examples) -> (X_all, Y_all, T_all)
      - prepare_dataloaders(X_all, Y_all, cfg) -> (dl_train, dl_val, n_train)
      - Pipeline(latent_dim, enc_hidden, dec_hidden) -> nn.Module
      - init_weights_xavier(model)
      - AdaptiveWeightDecay (callable class)
      - mixup_cutmix_batch(xb, yb, mixup_alpha, cutmix_alpha, prob, device) -> (mixed_x, mixed_y_or_None)
      - compute_majority_baseline_from_array(Y_all, n_train)
      - label_distribution_from_array(Y_all, n_train, N_COLORS)
      - GRID (int), N_COLORS (int), DEVICE (torch.device)
    """
    # --- Basic validation and defaults ---
    if stop_event is None:
        stop_event = threading.Event()

    # merge cfg over DEFAULT_CFG without modifying DEFAULT_CFG
    cfg = {**DEFAULT_CFG, **(cfg or {})}

    # deterministic seed if provided
    seed = cfg.get("seed", None)
    if seed is not None:
        np.random.seed(int(seed))
        torch.manual_seed(int(seed))
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(int(seed))

    # verify required globals exist in the environment, raise helpful errors if not
    env = globals()
    for name in ("build_dataset", "prepare_dataloaders", "Pipeline", "init_weights_xavier",
                 "mixup_cutmix_batch", "compute_majority_baseline_from_array", "label_distribution_from_array"):
        if name not in env:
            _ensure_required(name)
    # Optional class
    has_awd = "AdaptiveWeightDecay" in env

    # ensure GRID, N_COLORS, DEVICE exist
    if "GRID" not in env:
        _ensure_required("GRID")
    if "N_COLORS" not in env:
        _ensure_required("N_COLORS")
    if "DEVICE" not in env:
        _ensure_required("DEVICE")

    GRID = env["GRID"]
    N_COLORS = env["N_COLORS"]
    DEVICE = env["DEVICE"]

    emit("Building ARC dataset...")
    X_all, Y_all, T_all = env["build_dataset"](int(cfg.get("n_examples", DEFAULT_CFG["n_examples"])))
    dl_train, dl_val, n_train = env["prepare_dataloaders"](X_all, Y_all, cfg)

    emit(f"Train size: {len(dl_train.dataset)}  Val size: {len(dl_val.dataset)}")
    emit(f"Device: {DEVICE}")

    # baseline & distribution (guard with try)
    try:
        baseline = env["compute_majority_baseline_from_array"](Y_all, n_train)
        emit(f"Majority-class baseline (val) = {baseline:.2f}%")
    except Exception as ex:
        emit(f"Warning: could not compute majority baseline: {ex}")
        baseline = None

    try:
        distr = env["label_distribution_from_array"](Y_all, n_train, N_COLORS)
        emit("Validation label distribution (per-class counts): " + ", ".join(str(int(x)) for x in distr.tolist()))
    except Exception:
        pass

    # model, init, device
    model = env["Pipeline"](int(cfg.get("latent_dim")), int(cfg.get("encoder_hidden")), int(cfg.get("decoder_hidden")))
    try:
        env["init_weights_xavier"](model)
    except Exception:
        # non-fatal: some users prefer custom init; continue
        emit("init_weights_xavier not applied (missing or failed) â€” continuing with default init")

    model = model.to(DEVICE)

    optimizer = optim.AdamW(model.parameters(), lr=float(cfg.get("lr", 3e-4)), weight_decay=float(cfg.get("weight_decay", 1e-3)))
    # ensure T_max >= 1
    epochs = max(1, int(cfg.get("epochs", 8)))
    try:
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs)
    except Exception:
        scheduler = None

    # criterion: if label_smoothing provided and we use hard labels in validation, it's safe to construct CE with smoothing.
    use_label_smoothing = cfg.get("label_smoothing", None)
    if use_label_smoothing:
        criterion_ce = nn.CrossEntropyLoss(label_smoothing=float(use_label_smoothing))
    else:
        criterion_ce = nn.CrossEntropyLoss()

    # adaptive weight decay (optional)
    awd = None
    if cfg.get("adaptive_wd", False):
        if has_awd:
            awd_cls = env["AdaptiveWeightDecay"]
            awd = awd_cls(
                optimizer,
                base_wd=float(cfg.get("weight_decay", 1e-5)),
                ema_decay=float(cfg.get("wd_ema_decay", 0.98)),
                resp=float(cfg.get("wd_resp", 1.0)),
                wd_min=float(cfg.get("wd_min", 1e-6)),
                wd_max=float(cfg.get("wd_max", 1e-2)),
            )
        else:
            emit("AdaptiveWeightDecay requested but not available in environment; continuing without AWDecay.")
            awd = None

    # useful constants
    GRID_PIXELS = int(GRID * GRID)
    total_examples_processed = 0
    total_steps = 0
    start_time = time.time()

    # training loop
    for epoch in range(1, epochs + 1):
        if stop_event.is_set():
            emit(f"Stop requested before epoch {epoch}.")
            break

        model.train()
        total_train_pixel_loss = 0.0
        epoch_wd_accum = 0.0
        wd_steps = 0
        t0 = time.time()
        steps_this_epoch = 0

        for xb, yb in dl_train:
            if stop_event.is_set():
                break

            xb = xb.to(DEVICE)
            yb = yb.to(DEVICE)

            # Apply MixUp/CutMix - the helper may return (mixed_x, mixed_y) where mixed_y is either:
            # - a soft label tensor shaped (B*H*W, N_COLORS), or
            # - None meaning "no mixing was applied" (we then compute one-hot from yb)
            mixed_x, mixed_y = env["mixup_cutmix_batch"](
                xb, yb,
                mixup_alpha=float(cfg.get("mixup_alpha", 0.2)),
                cutmix_alpha=float(cfg.get("cutmix_alpha", 1.0)),
                prob=float(cfg.get("mix_prob", 0.5)),
                device=DEVICE
            )

            # If mixed_y is None, convert hard labels to one-hot soft targets
            B = xb.shape[0]
            if mixed_y is None:
                target_soft = _to_onehot(yb, N_COLORS, device=DEVICE, dtype=torch.float32)
            else:
                # assume mix function returned (B*H*W, N_COLORS) shaped soft targets already on-device or not
                target_soft = mixed_y.to(DEVICE) if not mixed_y.device == DEVICE else mixed_y

            optimizer.zero_grad()
            logits = model(mixed_x)  # expected (B, H, W, C)
            # flatten to (B*H*W, C)
            logits_flat = logits.view(B * GRID_PIXELS, N_COLORS)

            # soft-target loss: -sum(target * log_prob) averaged over pixels
            logp = F.log_softmax(logits_flat, dim=1)
            loss = -(target_soft * logp).sum(dim=1).mean()

            loss.backward()

            # adaptive WD step if present
            if awd is not None:
                try:
                    grad_norm, cur_wd = awd.step_update()
                    epoch_wd_accum += float(cur_wd)
                    wd_steps += 1
                except Exception as ex:
                    emit(f"Warning: AdaptiveWeightDecay.step_update failed: {ex}")

            optimizer.step()
            total_train_pixel_loss += float(loss.item()) * (B * GRID_PIXELS)
            total_examples_processed += (B * GRID_PIXELS)
            steps_this_epoch += 1
            total_steps += 1

        # scheduler step (epoch end)
        if scheduler is not None:
            try:
                scheduler.step()
            except Exception as ex:
                emit(f"Warning: scheduler.step() failed: {ex}")

        train_loss_per_pixel = total_train_pixel_loss / max(1, len(dl_train.dataset) * GRID_PIXELS)

        # validation
        model.eval()
        total_val_pixel_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for xb, yb in dl_val:
                if stop_event.is_set():
                    break
                xb = xb.to(DEVICE)
                yb = yb.to(DEVICE)
                Bv = xb.shape[0]
                logits = model(xb)
                vloss = criterion_ce(logits.view(Bv * GRID_PIXELS, N_COLORS), yb.view(-1))
                total_val_pixel_loss += float(vloss.item()) * (Bv * GRID_PIXELS)
                preds = logits.argmax(dim=-1)
                correct += int((preds == yb).sum().item())
                total += int(preds.numel())

        val_loss_per_pixel = total_val_pixel_loss / max(1, len(dl_val.dataset) * GRID_PIXELS)
        acc = 100.0 * correct / total if total > 0 else 0.0
        elapsed = time.time() - t0
        avg_wd = (epoch_wd_accum / wd_steps) if wd_steps > 0 else optimizer.param_groups[0].get('weight_decay', float(cfg.get("weight_decay", 1e-3)))

        # throughput metrics (guard elapsed > 0)
        epoch_examples = max(1, len(dl_train.dataset) * GRID_PIXELS)
        examples_per_sec = epoch_examples / elapsed if elapsed > 0 else float("inf")
        steps_per_sec = (steps_this_epoch / elapsed) if elapsed > 0 else float("inf")

        emit(f"[Epoch {epoch}/{epochs}] train_loss(pix)={train_loss_per_pixel:.6f} val_loss(pix)={val_loss_per_pixel:.6f} "
             f"val_acc={acc:.2f}% time={elapsed:.4f}s wd={avg_wd:.6g} examples/s={examples_per_sec:.2f} steps/s={steps_per_sec:.2f}")

        # optionally save checkpoint
        try:
            if cfg.get("save_every_n_epochs", 1) and (epoch % int(cfg.get("save_every_n_epochs", 1)) == 0):
                ckpt_path = os.path.join(cfg.get("checkpoint_dir", "checkpoints_arc"), f"ckpt_epoch{epoch}.pt")
                torch.save({
                    "epoch": epoch,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "cfg": cfg,
                }, ckpt_path)
                emit(f"Saved checkpoint: {ckpt_path}")
        except Exception as ex:
            emit(f"Warning: failed to save checkpoint: {ex}")

    duration = time.time() - start_time
    emit("ARC grid-to-grid training finished.")

    # build JSON-friendly config (convert numpy ints)
    safe_config = {}
    for k, v in cfg.items():
        if isinstance(v, (np.integer,)):
            safe_config[k] = int(v)
        else:
            try:
                # attempt json-friendly scalar conversion for numpy types
                if isinstance(v, (np.floating, np.bool_)):
                    safe_config[k] = v.item()
                else:
                    safe_config[k] = v
            except Exception:
                safe_config[k] = str(v)

    summary = {
        'run_started_at': time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start_time)),
        'duration_seconds': float(duration),
        'device': str(DEVICE),
        'config': safe_config,
        'total_examples_processed': int(total_examples_processed),
        'total_steps': int(total_steps),
        'final_val_accuracy_percent': float(acc),
    }
    return summary


# ------------------------
# Quick self-test (ARC-only)
# ------------------------
def quick_self_test_arc(emit=print):
    emit("Running quick self-test (strict ARC)...")
    try:
        cfg = dict(DEFAULT_CFG)
        cfg.update({
            "n_examples": 64,
            "batch_size": 8,
            "epochs": 2,
            "latent_dim": 64,
            "encoder_hidden": 64,
            "decoder_hidden": 64,
            "adaptive_wd": False,
            "num_workers": 0,
            "pin_memory": False,
            "mix_prob": 0.0,  # disable mixing for quick smoke-run by default
        })
        stop_event = threading.Event()
        summary = run_arc_tests(cfg, emit=emit, stop_event=stop_event)
        emit("Quick ARC summary: " + json.dumps(summary, indent=2))
        emit("Self-test completed successfully.")
    except Exception as e:
        emit("Self-test failed with exception:")
        emit(repr(e))
        traceback.print_exc()


# ------------------------
# GUI (Tkinter) â€” streamlined for ARC-only runs
# ------------------------
def launch_tk_gui():
    import tkinter as tk
    from tkinter import ttk, filedialog, messagebox
    from tkinter import scrolledtext
    import queue, threading, json, traceback, time, datetime, os

    
    def now():
        return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Simple tooltip class
    class ToolTip:
        def __init__(self, widget, text):
            self.widget = widget
            self.text = text
            self.tip = None
            widget.bind("<Enter>", self.show)
            widget.bind("<Leave>", self.hide)

        def show(self, _e=None):
            if self.tip or not self.text:
                return
            x, y, cx, cy = self.widget.bbox("insert") if hasattr(self.widget, "bbox") else (0,0,0,0)
            x += self.widget.winfo_rootx() + 20
            y += self.widget.winfo_rooty() + 10
            self.tip = tk.Toplevel(self.widget)
            self.tip.overrideredirect(True)
            lbl = tk.Label(self.tip, text=self.text, justify="left", background="#ffffe0", relief="solid", borderwidth=1, padx=4, pady=2)
            lbl.pack()

        def hide(self, _e=None):
            if self.tip:
                self.tip.destroy()
                self.tip = None

    # ---------- Main window ----------
    root = tk.Tk()
    root.title("ARC Strict Grid AIO â€” Click & Run")
    try:
        # small modern-ish geometry, allow resizing
        root.geometry("920x640")
        root.minsize(760, 480)
    except Exception:
        pass

    # ---------- Styles ----------
    style = ttk.Style(root)
    # Use default theme but tweak a few things for a sleeker look
    try:
        style.theme_use("clam")
    except Exception:
        pass
    style.configure("Header.TLabel", font=("Segoe UI", 11, "bold"))
    style.configure("TFrame", padding=6)
    style.configure("TButton", padding=6)
    style.configure("Small.TEntry", padding=4)
    style.configure("Info.TLabel", foreground="#2b6cb0")
    style.configure("Status.TLabel", foreground="#155724", background="#d4edda", padding=4)

    frm = ttk.Frame(root)
    frm.pack(fill="both", expand=True)

    # ---------- Menu ----------
    menubar = tk.Menu(root)
    file_menu = tk.Menu(menubar, tearoff=0)
    def save_log_to_file():
        try:
            path = filedialog.asksaveasfilename(defaultextension=".log", filetypes=[("Log files", "*.log"), ("Text files", "*.txt")])
            if not path:
                return
            content = log_box.get("1.0", "end").strip()
            with open(path, "w", encoding="utf-8") as f:
                f.write(content)
            emit_to_queue(f"Saved log to {os.path.basename(path)}")
        except Exception as e:
            messagebox.showerror("Save failed", repr(e))
    file_menu.add_command(label="Save Log...", command=save_log_to_file)
    file_menu.add_separator()
    file_menu.add_command(label="Exit", command=root.quit)
    menubar.add_cascade(label="File", menu=file_menu)

    help_menu = tk.Menu(menubar, tearoff=0)
    help_menu.add_command(label="About", command=lambda: messagebox.showinfo("About", "ARC Strict Grid AIO â€” Click & Run\nMade snazzier"))
    menubar.add_cascade(label="Help", menu=help_menu)
    root.config(menu=menubar)

    # ---------- Top: Run Configuration & Controls ----------
    top_pane = ttk.Frame(frm)
    top_pane.pack(fill="x", padx=8, pady=(8, 0))

    cfg_frame = ttk.LabelFrame(top_pane, text="Run Configuration")
    cfg_frame.pack(side="left", fill="x", expand=True, padx=(0,8))

    # Grid layout for config entries
    # Labels/entries
    ttk.Label(cfg_frame, text="n_examples:", style="Header.TLabel").grid(row=0, column=0, sticky="w", padx=6, pady=6)
    n_examples_var = tk.IntVar(value=DEFAULT_CFG.get("n_examples", 120))
    ttk.Entry(cfg_frame, textvariable=n_examples_var, width=10, style="Small.TEntry").grid(row=0, column=1, sticky="w", padx=2)

    ttk.Label(cfg_frame, text="Batch size:", style="Header.TLabel").grid(row=0, column=2, sticky="w", padx=6, pady=6)
    batch_var = tk.IntVar(value=DEFAULT_CFG.get("batch_size", 128))
    ttk.Entry(cfg_frame, textvariable=batch_var, width=8, style="Small.TEntry").grid(row=0, column=3, sticky="w", padx=2)

    ttk.Label(cfg_frame, text="Epochs:", style="Header.TLabel").grid(row=1, column=0, sticky="w", padx=6, pady=6)
    epochs_var = tk.IntVar(value=DEFAULT_CFG.get("epochs", 128))
    ttk.Entry(cfg_frame, textvariable=epochs_var, width=8, style="Small.TEntry").grid(row=1, column=1, sticky="w", padx=2)

    ttk.Label(cfg_frame, text="latent_dim:", style="Header.TLabel").grid(row=1, column=2, sticky="w", padx=6, pady=6)
    latent_var = tk.IntVar(value=DEFAULT_CFG.get("latent_dim", 512))
    ttk.Entry(cfg_frame, textvariable=latent_var, width=8, style="Small.TEntry").grid(row=1, column=3, sticky="w", padx=2)

    adaptive_var = tk.BooleanVar(value=DEFAULT_CFG.get("adaptive_wd", False))
    adaptive_cb = ttk.Checkbutton(cfg_frame, text="adaptive_wd", variable=adaptive_var)
    adaptive_cb.grid(row=2, column=0, padx=6, pady=6, sticky="w")
    ToolTip(adaptive_cb, "Enable adaptive weight decay during training")

    # Quick presets dropdown
    presets = {"Small test": {"n_examples": 32, "batch_size": 8, "epochs": 2, "latent_dim": 64},
               "Default": DEFAULT_CFG,
               "Full run": {"n_examples": 1200, "batch_size": 256, "epochs": 256, "latent_dim": 1024}}
    preset_var = tk.StringVar(value="Default")
    ttk.Label(cfg_frame, text="Preset:", style="Header.TLabel").grid(row=2, column=2, sticky="w", padx=6, pady=6)
    preset_menu = ttk.OptionMenu(cfg_frame, preset_var, "Default", *presets.keys(),
                                 command=lambda p: [n_examples_var.set(presets[p].get("n_examples", n_examples_var.get())),
                                                    batch_var.set(presets[p].get("batch_size", batch_var.get())),
                                                    epochs_var.set(presets[p].get("epochs", epochs_var.get())),
                                                    latent_var.set(presets[p].get("latent_dim", latent_var.get()))])
    preset_menu.grid(row=2, column=3, sticky="w", padx=2)

    # ---------- Toolbar (Start / Stop / Quick Test) ----------
    toolbar = ttk.Frame(top_pane)
    toolbar.pack(side="right", fill="y")

    start_btn = ttk.Button(toolbar, text="Start â–¶")
    start_btn.pack(side="top", pady=(6,4), padx=4, fill="x")
    # NOTE: stop button is intentionally AVAILABLE at all times (user requested)
    stop_btn = ttk.Button(toolbar, text="Stop â– ", state="normal")
    stop_btn.pack(side="top", pady=4, padx=4, fill="x")
    quick_btn = ttk.Button(toolbar, text="Quick Self-Test âœ¦")
    quick_btn.pack(side="top", pady=4, padx=4, fill="x")

    # Progress + status bar
    pb = ttk.Progressbar(toolbar, mode="determinate", maximum=100)
    pb.pack(side="top", padx=6, pady=(12,6), fill="x")
    status_lbl = ttk.Label(frm, text="Idle", style="Status.TLabel")
    status_lbl.pack(side="bottom", fill="x", padx=8, pady=6)

    # ---------- Logs (center) ----------
    mid_pane = ttk.Frame(frm)
    mid_pane.pack(fill="both", expand=True, padx=8, pady=(8,0))

    log_frame = ttk.LabelFrame(mid_pane, text="Logs")
    log_frame.pack(fill="both", expand=True)

    log_box = scrolledtext.ScrolledText(log_frame, state="disabled", wrap="word", height=18)
    log_box.pack(fill="both", expand=True, padx=4, pady=4)

    # Configure log text tags for colored levels
    log_box.tag_config("INFO", foreground="#0b5fff")
    log_box.tag_config("SUCCESS", foreground="#075e2d")
    log_box.tag_config("WARN", foreground="#7a5219")
    log_box.tag_config("ERROR", foreground="#b00020", underline=1)
    log_box.tag_config("TIMESTAMP", foreground="#6b7280", font=("Courier", 8))

    # ---------- Footer controls ----------
    footer = ttk.Frame(frm)
    footer.pack(fill="x", padx=8, pady=8)
    save_log_btn = ttk.Button(footer, text="Save Log")
    save_log_btn.pack(side="right")
    ToolTip(save_log_btn, "Save the current log to a file")

    # queue + threading state
    q = queue.Queue()
    stop_event = threading.Event()
    worker_thread = None

    def emit_to_queue(msg: str, level="INFO"):
        # allow emitting structured progress messages like "PROGRESS:45"
        q.put(("MSG", msg, level))

    def emit_progress(pct: float):
        q.put(("PROGRESS", pct))

    def poll_queue():
        # Read queue and update UI
        try:
            while True:
                item = q.get_nowait()
                if item[0] == "MSG":
                    _, msg, level = item
                    # Special UI restore message (run finished / interrupted)
                    if msg == "UI:restore":
                        # Re-enable configuration widgets
                        for child in cfg_frame.winfo_children():
                            try:
                                child.configure(state="normal")
                            except Exception:
                                pass
                        # restore main buttons and progress/status
                        start_btn.config(state="normal")
                        # keep stop button available for user as requested
                        stop_btn.config(state="normal")
                        pb['value'] = 0
                        status_lbl.config(text="Idle")
                        stop_event.clear()
                        q.task_done()
                        continue

                    # normal log messages
                    log_box.configure(state="normal")
                    log_box.insert("end", f"{now()} | ", "TIMESTAMP")
                    log_box.insert("end", f"{msg}\n", level)
                    log_box.see("end")
                    log_box.configure(state="disabled")
                    # quick visual status
                    if level == "ERROR":
                        status_lbl.config(text="Error", style="Status.TLabel")
                    elif level == "WARN":
                        status_lbl.config(text="Warning", style="Status.TLabel")
                    else:
                        status_lbl.config(text="Running" if start_btn['state']=="disabled" else "Idle", style="Status.TLabel")
                elif item[0] == "PROGRESS":
                    _, pct = item
                    try:
                        pb['value'] = float(pct)
                    except Exception:
                        pass
                q.task_done()
        except queue.Empty:
            pass
        root.after(150, poll_queue)

    def make_cfg():
        return {
            "n_examples": int(n_examples_var.get()),
            "batch_size": int(batch_var.get()),
            "epochs": int(epochs_var.get()),
            "latent_dim": int(latent_var.get()),
            "encoder_hidden": int(max(64, latent_var.get())),
            "decoder_hidden": int(max(64, latent_var.get())),
            "lr": 3e-4,
            "weight_decay": 1e-3,
            "num_workers": 0,
            "pin_memory": True,
            "checkpoint_dir": "checkpoints_arc",
            "adaptive_wd": bool(adaptive_var.get()),
            "wd_ema_decay": 0.98,
            "wd_resp": 1.0,
            "wd_min": 1e-6,
            "wd_max": 1e-2,
            "train_val_split": 0.8,
            "mixup_alpha": 0.2,
            "cutmix_alpha": 1.0,
            "mix_prob": 0.5,
            "label_smoothing": None,
        }

    # Worker wrapper expects run_arc_tests(cfg_local, emit=emit_to_queue, stop_event=stop_event)
    def worker_fn(cfg_local):
        nonlocal worker_thread, stop_event
        try:
            emit_to_queue("Starting ARC strict run...", "INFO")
            # If run_arc_tests supports a progress callback, give it a small wrapper:
            def progress_cb(pct):
                try:
                    emit_progress(pct)
                except Exception:
                    pass
            # call run function; allow it to call emit_to_queue for messages
            summary = run_arc_tests(cfg_local, emit=emit_to_queue, stop_event=stop_event, progress_cb=progress_cb) \
                if "progress_cb" in run_arc_tests.__code__.co_varnames else run_arc_tests(cfg_local, emit=emit_to_queue, stop_event=stop_event)
            emit_to_queue("ARC run finished â€” summary: " + json.dumps(summary), "SUCCESS")
        except Exception as e:
            emit_to_queue("ARC failure: " + repr(e), "ERROR")
            traceback.print_exc()
        finally:
            # restore UI state on main thread via queue
            q.put(("MSG", "UI:restore", "INFO"))

    def on_start():
        nonlocal worker_thread, stop_event
        # disable start-only inputs so user can't start twice; keep stop available always
        start_btn.config(state="disabled")
        for child in cfg_frame.winfo_children():
            try:
                child.configure(state="disabled")
            except Exception:
                pass
        stop_event.clear()
        cfg_local = make_cfg()
        emit_to_queue("Prepared configuration: " + json.dumps({k: cfg_local[k] for k in ("n_examples","batch_size","epochs","latent_dim")}), "INFO")
        worker_thread = threading.Thread(target=worker_fn, args=(cfg_local,), daemon=True)
        worker_thread.start()
        status_lbl.config(text="Running")

    def on_stop():
        # Always allow stop; don't fully disable it so user can try again if needed
        stop_event.set()
        emit_to_queue("Stop requested â€” attempting graceful interruption...", "WARN")
        # do NOT disable stop_btn; keep it available as a safety control

    def on_quick_test():
        def _qt():
            try:
                emit_to_queue("Starting quick self-test (ARC-only)...", "INFO")
                quick_self_test_arc(emit=emit_to_queue)
                emit_to_queue("Quick self-test finished.", "SUCCESS")
            except Exception as e:
                emit_to_queue("Quick self-test failed: " + repr(e), "ERROR")
                traceback.print_exc()
        # disable relevant buttons briefly
        quick_btn.config(state="disabled")
        threading.Thread(target=lambda: ( _qt(), root.after(100, lambda: quick_btn.config(state="normal")) ), daemon=True).start()

    # Save log button
    save_log_btn.config(command=save_log_to_file)

    # configure button commands
    start_btn.config(command=on_start)
    stop_btn.config(command=on_stop)
    quick_btn.config(command=on_quick_test)

    # Keyboard shortcuts
    root.bind_all("<Control-s>", lambda e: save_log_to_file())
    root.bind_all("<Control-q>", lambda e: root.quit())

    # Start the queue poller
    root.after(150, poll_queue)
    root.mainloop()




# ------------------------
# CLI
# ------------------------
def main_cli():
    parser = argparse.ArgumentParser(description="ARC Strict Grid AIO â€” runs only ARC grid-to-grid tests")
    parser.add_argument("--mode", choices=["gui", "cli"], default="gui", help="Run mode (gui or cli)")
    parser.add_argument("--quick-test", action="store_true", help="Run a quick self-test (smoke-check)")
    parser.add_argument("--n_examples", type=int, default=DEFAULT_CFG["n_examples"])
    parser.add_argument("--batch_size", type=int, default=DEFAULT_CFG["batch_size"])
    parser.add_argument("--epochs", type=int, default=DEFAULT_CFG["epochs"])
    parser.add_argument("--latent_dim", type=int, default=DEFAULT_CFG["latent_dim"])
    parser.add_argument("--adaptive_wd", action="store_true", help="Enable adaptive weight decay")
    args = parser.parse_args()

    if args.mode == "gui":
        try:
            launch_tk_gui()
            return
        except Exception as e:
            print("GUI failed to launch:", e)
            traceback.print_exc()
            # fall through to CLI

    if args.quick_test:
        quick_self_test_arc(emit=print)
        return

    # CLI strict ARC run
    cfg = dict(DEFAULT_CFG)
    cfg["n_examples"] = args.n_examples
    cfg["batch_size"] = args.batch_size
    cfg["epochs"] = args.epochs
    cfg["latent_dim"] = args.latent_dim
    cfg["encoder_hidden"] = max(64, args.latent_dim)
    cfg["decoder_hidden"] = max(64, args.latent_dim)
    cfg["adaptive_wd"] = bool(args.adaptive_wd)

    print("CUDA available:", torch.cuda.is_available())
    if torch.cuda.is_available():
        try:
            print("CUDA device:", torch.cuda.get_device_name(0))
        except Exception:
            pass

    print("Starting strict ARC run with config:", {k: cfg[k] for k in ("n_examples", "latent_dim", "encoder_hidden", "decoder_hidden", "batch_size", "epochs")})
    try:
        summary = run_arc_tests(cfg, emit=print, stop_event=threading.Event())
        print("Run summary:")
        print(json.dumps(summary, indent=2))
    except KeyboardInterrupt:
        print("Run interrupted by user (KeyboardInterrupt).")
    except Exception as e:
        print("Run failed:", repr(e))
        traceback.print_exc()


if __name__ == "__main__":
    main_cli()
